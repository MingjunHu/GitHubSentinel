# ollama 项目进展

## 时间周期：2024-09-18至2024-09-19

## 新增功能
- 为 `bespoke-minicheck` 添加 Python 示例
  
## 主要改进
- 针对以下问题进行了修复：
  - 修复了系统提示在 qwen2.5 上无法使用的问题 (#6873)
  - 解决了 Ollama 导致 Radeon 显示驱动崩溃的问题 (#6871)
  - 修复了在 Windows 上无法下载模型的问题 (#6861)
  - 处理了因 VRAM 较小的 GPU 运行失败的问题 (#6860)
  - 修复了构建过程中无法强制使用 CUDA/CUBLAS 的问题 (#6859)
  - 解决了与 rocm 相关的依赖获取问题 (#6848)

## 修复问题
- 解决了返回错误上下文信息过旧的问题 (#6838)
- 修复了 Solar Pro 的相关问题 (#6819)
- 针对 Pixtral-12b from Mistral 的问题进行了修复 (#6812)
- 处理了数学问题的错误回应 (#6794)
- 询问 OpenAI 工具流支持的即将到来 (#6790)
- 为模型停止操作提供文档 (#6766)
- 解决了无法执行 `mattw/loganalyzer` 的问题 (#6750)
- 处理了 HTTP_PROXY 未在模型请求中使用的问题 (#6679)
- 解决了 "max retries exceeded" 的错误 (#6211)
- 针对 GPU 上的层卸载提出了建议 (#5843)
- 解决了 Llama3 如何处理超出上下文窗口的对话的问题 (#5480)
- 处理了创建模型时出现的 "transferring model data" 错误 (#4998)
- 解决了与 bge-reranker-v2-m3 和 mxbai-rerank-large-v1 等重排模型相关的问题 (#4360)
- 处理了 OLLAMA_MODELS 目录的问题 (#2574)
- 支持删除部分下载的模型 (#1599)
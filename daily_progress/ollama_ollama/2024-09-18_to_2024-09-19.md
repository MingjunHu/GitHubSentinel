# Progress for ollama/ollama (2024-09-18 to 2024-09-19)


## Issues Closed in the Last 1 Days
- system prompt does not work on qwen2.5 #6873
- Ollama crashes the Radeon display driver #6871
- Qwen2.5 #6862
- Unable to pull models in Windows #6861
- It used to work on 4 GPUs with 12.2MiG and one GPU with 4MiB, now it dies on the smaller VRAM GPU #6860
- Something got changed in the build process and I seem unable to force CUDA/CUBLAS use. #6859
- llama: gather transitive dependencies for rocm for dist packaging #6848
- Add python examples for `bespoke-minicheck` #6841
- Old Context Information fetched #6838
- Solar Pro #6819
- Pixtral-12b from Mistral #6812
- Wrong response at math question! #6794
- openai tools streaming support coming soon? #6790
- documentation for stopping a model #6766
- mattw/loganalyzer 无法ollama run #6750
- 模型mattw/loganalyzer，按照example/python_loganalysis中的readme操作，但是http://localhost:11434/api/generate一直404 #6745
- HTTP_PROXY Not Being Used in Model Requests #6679
- Error: max retries exceeded #6211
- How to offload all layers to GPU? #5843
- How Does Llama3 Handle Dialogs Exceeding the Context Window? #5480
- Error "transferring model data " when creating a model #4998
- bge-reranker-v2-m3、mxbai-rerank-large-v1 and other rerank models #4360
- OLLAMA_MODELS Directory #2574
- Delete partially downloaded models. #1599

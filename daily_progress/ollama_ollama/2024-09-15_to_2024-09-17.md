# Progress for ollama/ollama (2024-09-15 to 2024-09-17)


## Issues Closed in the Last 2 Days
- CI: dist directories no longer present #6834
- CI: clean up naming, fix tagging latest #6832
- cache: Clear old KV cache entries when evicting a slot #6831
- llama: doc: explain golang objc linker warning #6830
- CI: set platform build build_linux script to keep buildx happy #6829
- fix typo in import docs #6828
- GPU Support for older CPUs lacking AVX #6827
- How to remove this #6824
- 使用Modelfile加载本地gguf文件，会胡乱输出内容 #6821
- Typo in Gemma 2 model card #6820
- Add vim-intelligence-bridge to Terminal section  in README #6818
- High GPU and CPU usage #6816
- how do l install the model to D hard drive? #6813
- Pixtral-12b from Mistral #6812
- Create docker-image.yml #6810
- 整理大量带中文标点的文本时，意外中断（重启解决了，可能是版本冲突） #6809
- add Agents-Flex Libraries in README.md #6788
- llama: opt-in at build time #6570
- ollama collapses CPU #6525
- Error: llama runner process has terminated: error loading model: unable to allocate backend buffer #5958
- Llama3.1 70b-instruct-q4_1 buggy #5918
- ollama run deepseek-coder-v2 creates gibberish output #5787
- Per-Model Concurrency #5693
- allow for num_ctx parameter in the openai API compatibility #5356
- Ignoring env, being weird with env #4771
- Phi-3 Vision #4591
- Not compiled with GPU offload support #4486
- error loading model: error loading model vocabulary: unknown pre-tokenizer type: 'qwen2' #4457
- Massive performance regression on 0.1.32 #3938
- Digest mismatch, file must be downloaded again #3931
